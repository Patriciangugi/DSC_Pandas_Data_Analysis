{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load customer demographics and claims information into separate dataframes:\n",
    "- customer demographics should be loaded into `cust_df` from 'cust_demographics.csv'\n",
    "- claims should be loaded into `claims_df` from 'claims.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect the DataFrames to get an understanding of the data, dtypes, and column structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check for duplicates and then remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Join the dataframes and save to a dataframe `combined_df`. Identify what column(s) you will join on and then use an inner join. Drop any redundant columns resulting from merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Identify cleaning tasks in the `claim_amount` column and execute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Clean fradulent and convert to appropriate numeric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Use an appropriate plot to visualize the the mean claim amount for each incidence cause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Plot the distribution of claim amounts: use a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. There is a bimodal distribution here. Create a binary column called \"claim_size\" that is \"large\" when the claim amount is greater than or equal to 10000 and \"small\" otherwise. We will use this binary categorical later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Investigate the fraudulent claim rate by state:\n",
    "- get the rate of fraudulent claims by state sorted in descending order. Save this to a Series `sorted_rates`\n",
    "- get the states with the top 15 fraudulent claim rates and save this list of state to `state_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. It would be interesting to see whether there is a differentiation of fraud rates by claim size in each state. First compute this by using a pivot table. Display results from the states with the top 15 highest fraudulent claim rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Conduct a groupby/aggregation(s) by State and claim size. This time you will compute the fraudulent claim rate as well as the number of examples in each category and subcategory. The result should be presented in a single DataFrame.\n",
    "\n",
    "*Hint*: you can compute each aggregation separately and then concatenate **or** you can look up how to perform multiple aggregations with the .agg() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not too many examples within each subgroup -- but it is plausible that there are enough here to differentiate between fraud behavior for small and large claims in many of the states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13. Plot fraudulent rates amongst small and large claim sizes in the 15 states with the top average fraudulent rates:\n",
    "- put this in a single bar chart with large and small claim sizes overlayed on top of each other\n",
    "- use blue for large claim and orange for small claim, set transparency parameter to 0.4.\n",
    "- make sure that you label plots appropriately and display legends.\n",
    "\n",
    "*Hint*: selecting the appropriate column and unstacking the results of your groubpy on State and claim size might be helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Just because its a lot of fun and good for you, melt the pivot table with average fraudulent rates (pivoted on State and claim size) for the states with top 15 highest overall fradulent rates. We have recalculated the table that is to be melted below from `combined_df` and have stored it in `widestateclaims`:\n",
    "- make sure that the index is still State (look up what option for melt you need)\n",
    "- the value should be named 'fraud_rate'\n",
    "- save to `melted_claims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. [optional] There are a lot of other aspects of the data that we have not touched. In particular, it might be interesting to look at how claimant age and gender influence claim behavior. You could also take a look at incident cause and whether a police report was filed or not in seeing how fraudulent claims and claim size are influenced by these two categorical classes. Knock yourself out and use your reshaping and visualization skills to explore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
